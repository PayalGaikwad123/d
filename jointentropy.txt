function z = entropy(x)n = numel(x);
[u,~,x] = unique(x);
k = numel(u);
idx = 1:n;
Mx = sparse(idx,x,1,n,k,n);
Px = nonzeros(mean(Mx,1));
Hx = -dot(Px,log2(Px));
z = max(0,Hx);
end


function z = jointEntropy(x, y)
assert(numel(x) == numel(y));
n = numel(x);
x = reshape(x,1,n);
y = reshape(y,1,n);
l = min(min(x),min(y));
x = x-l+1;
y = y-l+1;
k = max(max(x),max(y));
idx = 1:n;
p = nonzeros(sparse(idx,x,1,n,k,n)'*sparse(idx,y,1,n,k,n)/n);   %joint distribution of x and y
z = -dot(p,log2(p));
z = max(0,z);
end

k = 10;  % variable range
n = 100;  % number of variables
x = ceil(k*rand(1,n));
y = ceil(k*rand(1,n));
% x = randi(k,1,n);  % need statistics toolbox
% y = randi(k,1,n);
%% Entropy H(x), H(y)
Hx = entropy(x);
Hy = entropy(y);
%% Joint entropy H(x,y)
Hxy = jointEntropy(x,y);


disp( ‘= = = = =’)
disp (Hxy)